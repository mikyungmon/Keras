{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ 확률분포를 생성하는 완전 연결 계층 GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0 (Epoch: 0)\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": " Error while reading resource variable _AnonymousVar511 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar511/class tensorflow::Var does not exist.\n\t [[node mul_254/ReadVariableOp (defined at C:\\Users\\mkflo\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_17693]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmachine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMachine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mni_D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 매 에포크 마다 길이가 100인 벡터 하나를 출력하도록 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmachine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_show\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# repeat : 전체 반복 횟수, show : 결과 표시할 총 에포크 수, test : 성능 평가 시 사용할 샘플 수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, n_repeat, n_show, n_test)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stage'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'(Epoch: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mii\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mn_show\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_show\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m             \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mrun_epochs\u001b[1;34m(self, epochs, n_test)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# 에포크 단위 실행 멤버 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_and_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# GAN 학습 진행하는 멤버 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_each\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mtrain_each\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_each\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 매순간 학습 진행 멤버 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 판별망은 n_iter_D만큼 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_G\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 학습용 생성망은 n_iter_G만큼 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mtrain_D\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mGen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 입력 샘플을 생성기에 통과시켜 생성망의 출력으로 바꿈\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m    \u001b[1;31m# 판별망은 학습용 생성망을 학습할 때는 학습이 되지 않도록 하고 학습 진행해야 함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD_train_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGen\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 판별망 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# 학습용 생성망 학습 멤버 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5f46bd7e5d72>\u001b[0m in \u001b[0;36mD_train_on_batch\u001b[1;34m(self, Real, Gen)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mGen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mGD_train_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# 왜 Gen이 아니라 Z?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m:  Error while reading resource variable _AnonymousVar511 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar511/class tensorflow::Var does not exist.\n\t [[node mul_254/ReadVariableOp (defined at C:\\Users\\mkflo\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_17693]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# 패키지 임포트\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models\n",
    "from keras.layers import Dense,Conv1D, Reshape, Flatten, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "# 코드 수행 결과 보기\n",
    "def main():\n",
    "    machine = Machine(n_batch = 1, ni_D = 100)   # 매 에포크 마다 길이가 100인 벡터 하나를 출력하도록 설정\n",
    "    machine.run(n_repeat = 200, n_show = 200, n_test = 100)   # repeat : 전체 반복 횟수, show : 결과 표시할 총 에포크 수, test : 성능 평가 시 사용할 샘플 수 \n",
    "    \n",
    "\n",
    "# 데이터 관리 클래스 \n",
    "class Data:\n",
    "    def __init__(self,mu,sigma, ni_D):\n",
    "        self.real_sample = lambda n_batch: np.random.normal(mu,sigma,(n_batch, ni_D))   # 흉내 내고자 하는 실제 데이터\n",
    "        self.in_sample = lambda n_batch: np.random.rand(n_batch,ni_D)    # 무작위 잡음 데이터 \n",
    "        \n",
    "# 머신 구현하기\n",
    "class Machine:\n",
    "    def __init__(self,n_batch = 10,ni_D = 100):   # 클래스 초기화 함수\n",
    "        data_mean = 4\n",
    "        data_stddev = 1.25\n",
    "        \n",
    "        self.n_iter_D = 1\n",
    "        self.n_iter_G = 5\n",
    "        \n",
    "        self.data = Data(data_mean, data_stddev, ni_D)    # ni_D : 판별망이 한꺼번에 받아들일 확률변수 수\n",
    "        self.gan = GAN(ni_D = ni_D , nh_D = 50, nh_G = 50)\n",
    "        \n",
    "        self.n_batch = n_batch\n",
    "        \n",
    "    def train_D(self):   # 판별망 학습 멤버 함수\n",
    "        gan = self.gan\n",
    "        n_batch = self.n_batch\n",
    "        data = self.data\n",
    "        \n",
    "        # real data\n",
    "        Real = data.real_sample(n_batch)   # 실제 데이터에서 n_batch만큼 샘플을 가져옴\n",
    "        # generated data\n",
    "        Z = data.in_sample(n_batch)  # 임의의 분포를 가지는 입력 샘플을 데이터 샘플 수와 같은 수 만큼 만듦\n",
    "        Gen = gan.G.predict(Z)   # 입력 샘플을 생성기에 통과시켜 생성망의 출력으로 바꿈\n",
    "        gan.D.trainable = True    # 판별망은 학습용 생성망을 학습할 때는 학습이 되지 않도록 하고 학습 진행해야 함\n",
    "        gan.D_train_on_batch(Real,Gen)   # 판별망 학습\n",
    "        \n",
    "    def train_GD(self):   # 학습용 생성망 학습 멤버 함수\n",
    "        gan = self.gan\n",
    "        n_batch = self.n_batch\n",
    "        data = self.data\n",
    "        Z = data.in_sample(n_batch)\n",
    "        \n",
    "        gan.D.trainable = False     # 판별망은 학습용 생성망을 학습할 때는 학습이 되지 않도록 하고 학습 진행해야 함\n",
    "        gan.GD_train_on_batch(Z)    # 입력이 생성망에 들어가면 모든 판별망이 실제 샘플로 착각하게 학습\n",
    "        \n",
    "    def train_each(self):    # 매순간 학습 진행 멤버 함수\n",
    "        for it in range(self.n_iter_D):    # 판별망은 n_iter_D만큼 학습\n",
    "            self.train_D()\n",
    "        for it in range(self.n_iter_G):    # 학습용 생성망은 n_iter_G만큼 학습\n",
    "            self.train_GD()\n",
    "            \n",
    "    def train(self,epochs):    # GAN 학습 진행하는 멤버 함수\n",
    "        for epoch in range(epochs):\n",
    "            self.train_each()\n",
    "            \n",
    "    def test(self,n_test):\n",
    "        gan = self.gan\n",
    "        data = self.data\n",
    "        Z = data.in_sample(n_test)\n",
    "        Gen = gan.G.predict(Z)\n",
    "        return Gen,Z\n",
    "    \n",
    "    def show_hist(self,Real,Gen,Z):   # 학습 진행 경과에 대한 그래프 그리는 멤버 함수\n",
    "        plt.hist(Real.reshape(-1),histtype='step', label='Real')\n",
    "        plt.hist(Gen.reshape(-1),histtype='step', label='Generated')\n",
    "        plt.hist(Z.reshape(-1),histtype = 'step', label ='Input')\n",
    "        plt.legend(loc=0)\n",
    "        \n",
    "    def test_and_show(self,n_test):   # 성능 평가 및 그래프 그리기 멤버 함수\n",
    "        data= self.data\n",
    "        Gen,Z = self.test(n_test)\n",
    "        Real = data.real_sample(n_test)\n",
    "        self.show_hist(Real,Gen,Z)\n",
    "        Machine.print_stat(Real,Gen)\n",
    "        \n",
    "    def run_epochs(self,epochs,n_test):   # 에포크 단위 실행 멤버 함수\n",
    "        self.train(epochs)\n",
    "        self.test_and_show(n_test)\n",
    "        \n",
    "    def run(self, n_repeat=200, n_show=200, n_test=100):  # 실행 멤버 함수 , n_show번 학습이 진행될 때마다 그 결과를 그래프로 표시\n",
    "        for ii in range(n_repeat):\n",
    "            print('Stage', ii,'(Epoch: {})'.format(ii *n_show))\n",
    "            self.run_epochs(n_show, n_test)\n",
    "            plt.show()\n",
    "            \n",
    "    def print_stat(Real,Gen):   # 생성망이 얼마나 실제 데이터의 확률분포를 따르는 데이터를 만드는지 확인하는 정적 멤버 함수\n",
    "        def stat(d):\n",
    "            return (np.mean(d),np.std(d))\n",
    "        print('Mean and Std of Real:', stat(Real))\n",
    "        print('Mean and Std of Gen:', stat(Gen))\n",
    "        \n",
    "# GAN 모델링\n",
    "def add_decorate(x):   # 람다 계층의 처리 함수, 입력 벡터에 새로운 벡터 추가\n",
    "    m = K.mean(x,axis = -1, keepdims=True)\n",
    "    d = K.square(x-m)\n",
    "    return K.concatenate([x,d],axis=-1)\n",
    "\n",
    "def add_decorate_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 2\n",
    "    shape[1] *= 2\n",
    "    return tuple(shape)\n",
    "\n",
    "lr = 2e-4  # 0.0002\n",
    "adam = Adam(lr = lr, beta_1 = 0.9, beta_2 = 0.999)\n",
    "\n",
    "def model_compile(model):\n",
    "    return model.compile(loss='binary_crossentropy', optimizer=adam, metrics = ['accuracy'])\n",
    "\n",
    "class GAN :\n",
    "    def __init__(self,ni_D,nh_D,nh_G):\n",
    "        self.ni_D = ni_D\n",
    "        self.nh_D = nh_D\n",
    "        self.nh_G = nh_G\n",
    "        \n",
    "        self.D = self.gen_D()\n",
    "        self.G = self.gen_G()\n",
    "        self.GD = self.make_GD()\n",
    "        \n",
    "    def gen_D(self):\n",
    "        ni_D = self.ni_D\n",
    "        nh_D = self.nh_D\n",
    "        D = models.Sequential()\n",
    "        D.add(Lambda(add_decorate,output_shape = add_decorate_shape, input_shape = (ni_D,)))\n",
    "        D.add(Dense(nh_D, activation='relu'))\n",
    "        D.add(Dense(nh_D, activation='relu'))\n",
    "        D.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model_compile(D)\n",
    "        return D\n",
    "    \n",
    "    def gen_G(self):\n",
    "        ni_D = self.ni_D\n",
    "        nh_G = self.nh_D\n",
    "        \n",
    "        G = models.Sequential()\n",
    "        G.add(Reshape((ni_D,1), input_shape=(ni_D,)))\n",
    "        G.add(Conv1D(nh_G,1,activation='relu'))\n",
    "        G.add(Conv1D(nh_G,1,activation='sigmoid'))\n",
    "        G.add(Conv1D(1,1))\n",
    "        G.add(Flatten())\n",
    "        \n",
    "        model_compile(G)\n",
    "        return G\n",
    "    \n",
    "    def make_GD(self):\n",
    "        G,D = self.G, self.D\n",
    "        GD = models.Sequential()\n",
    "        GD.add(G)\n",
    "        GD.add(D)\n",
    "        D.trainable=False  # D가 학습되지 않게 trainable을 끔\n",
    "        model_compile(GD)\n",
    "        D.trainable=True\n",
    "        return GD\n",
    "    \n",
    "    def D_train_on_batch(self,Real,Gen):\n",
    "        D = self.D\n",
    "        X = np.concatenate([Real,Gen],axis=0)\n",
    "        y = np.array([1]*Real.shape[0] + [0] * Gen.shape[0])\n",
    "        D.train_on_batch(X,y)\n",
    "        \n",
    "    def GD_train_on_batch(self,Z):   # 왜 Gen이 아니라 Z?\n",
    "        GD = self.GD\n",
    "        y = np.array([1]*Z.shape[0])  # 생성망에서 출력되는 허구값을 판별망에서 실제값으로 판별하도록 학습해야하기 때문에 목표 출력값을 모두 1로 설정\n",
    "        GD.train_on_batch(Z,y)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ 필기체를 생성하는 합성곱 계층 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function image_data_format at 0x000001E625A0CE18>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--output_fold OUTPUT_FOLD]\n",
      "                             [--input_dim INPUT_DIM] [--n_train N_TRAIN]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\mkflo\\AppData\\Roaming\\jupyter\\runtime\\kernel-dbc058b0-40f1-4a07-bac5-8e87d0bcb75e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkflo\\.conda\\envs\\keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 공통 패키지 불러오기\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "K.set_image_data_format('channels_first')  # 이미지 데이터의 채널이 들어 있는 차원이 1번째가 되도록 설정\n",
    "print(K.image_data_format)\n",
    "\n",
    "# 신경망의 출력이 스칼라나 벡터가 아니라 다차원일 경우에는 해당 차원에 맞는 손실함수가 필요 -> 사용자가 직접 만듦\n",
    "# 4차원 데이터를 이용하는 손실 함수를 케라스 백엔드와 텐서플로로 구현\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "# 합성곱 계층 GAN 수행\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()  # 인자값 받을 수 있는 인스턴스 생성\n",
    "    \n",
    "    # 입력받을 인자값 등록\n",
    "    parser.add_argument('--batch_size', type = int, default = 16, help = 'Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type = int, default = 1000, help = 'Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type = str, default = 'GAN_OUT', help = 'Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type = int, default = 10, help = 'Input dimension for the generator')\n",
    "    parser.add_argument('--n_train',type = int, default = 32, help = 'The number of training data')\n",
    "    \n",
    "    args = parser.parse_args()  # 입력받은 인자들을 args에 저장\n",
    "    \n",
    "    train(args)\n",
    "    \n",
    "# 합성곱 계층 GAN 모델링\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim = 64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        \n",
    "        # 학습을 위한 생성망은 생성망과 판별망이 결합된 형태(학습용 생성망)\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False   # 결합 시 판별망 쪽은 학습이 진행되지 않도록 만듦. 학습용 생성망을 학습할 때는 이미 학습된 판별망을 사용하기 때문\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "        \n",
    "    def compile_all(self):  # 전체 신경망을 컴파일하는 함수\n",
    "        d_optim = optimizers.SGD(lr = 0.0005, momentum = 0.9, nesterov = True)\n",
    "        g_optim = optimizers.SGD(lr = 0.0005, momentum = 0.9, nesterov=True)\n",
    "        \n",
    "        self.generator.compile(loss = mse_4d_tf, optimizer = \"SGD\")  # 학습용 생성망 컴파일\n",
    "        self.compile(loss = 'binary_crossentropy',optimizer = g_optim)   # 순수 생성망은 학습 시 최적화 인스턴스인 g_optim을 사용해 최적화\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss = 'binary_crossentropy', optimizer = d_optim)\n",
    "        \n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation = 'tanh', input_dim = input_dim))   # 입력은 1차원 행렬\n",
    "        model.add(layers.Dense(128 * 7 * 7 , activation = 'tanh'))  # 완전 연결 계층 이용해 6772로 더 확장\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Reshape((128,7,7), input_shape = (128 * 7 * 7,)))  # 이미지의 채널, 가로 및 세로를 128 * 7 * 7 모양의 3차원 행렬로 재조정\n",
    "        model.add(layers.UpSampling2D(size = (2,2)))  # 이미지를 (2,2)배 확장\n",
    "        model.add(layers.Conv2D(64, (5,5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size = (2,2)))\n",
    "        model.add(layers.Conv2D(1,(5,5), padding='same', activation='tanh'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def DISCRIMINATOR(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64,(5,5), padding = 'same', activation='tanh', input_shape = (1,28,28)))   # 64 * 28 * 28 \n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2)))    # 64 * 14 * 14 \n",
    "        model.add(layers.Conv2D(128,(5,5),activation='tanh'))   # 128 * 14 * 14\n",
    "        model.add(layers.Maxpooling2D(pool_size = (2,2)))    # 128 * 7 * 7 \n",
    "        model.add(layers.Flatten())   # 3차원 -> 1차원 \n",
    "        model.add(layers.Dense(1024,activation='tanh'))\n",
    "        model.add(layers.Dense(1,activation='sigmoid'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_z(self,ln):  # 주어진 input_dim만큼 원소를 가지는 무작위 벡터를 만듦\n",
    "        input_dim = self.input_dim  \n",
    "        return np.random.uniform(-1,1,(ln,input_dim))  # 배치 크기인 ln으로 무작위 벡터 수를 설정\n",
    "    \n",
    "    def train_both(self,x):  # 모델들의 학습을 수행하는 함수\n",
    "        ln = x.shape[0]  # 배치 크기\n",
    "        \n",
    "        # first trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose = 0)\n",
    "        xw = np.concatenate((x,w))  # 실제 이미지들과 허구 이미지들을 데이터셋 하나로 합침\n",
    "        y2 = [1] * ln + [0] * ln\n",
    "        d_loss = self.discriminator.train_on_batch(xw,y2)\n",
    "        \n",
    "        # second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.disciminator.trainable = False\n",
    "        g_loss = self.generator.train_on_batch(z,[1]*ln)\n",
    "        self.discriminator.trainable = True\n",
    "        \n",
    "        return d_loss, g_loss\n",
    "    \n",
    "# 합성곱 계층 GAN 학습\n",
    "def combine_images(generated_images):\n",
    "    num = generated_iamges.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/ width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height *shape[0], width *shape[1]), dtype=generated_images.dtype)\n",
    "    \n",
    "    for index,img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0], j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "        \n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE : (index +1) *BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.unit8)).save(output_ford+'/',str(epoch)+\"_\" +str(index)+\".png\")\n",
    "    \n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_,_) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold    # 학습과정에서 생성된 이미지 중 일부를 간헐적으로 출력하는 폴더의 이름\n",
    "    input_dim = args.input_dim   # 무작위 벡터의 길이\n",
    "  \n",
    "    os.makedirs(output_fold, exist_ok = True)\n",
    "    print('Output_fold is', output_fold)    \n",
    "    \n",
    "    X_train = load_data(n_train)\n",
    "    \n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0],1)+ X_train.reshape[1:])\n",
    "    \n",
    "    gan = GAN(input_dim)\n",
    "    \n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        \n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train,index, BATCH_SIZE)   # x : 배치 크기 만큼의 입력 데이터\n",
    "  \n",
    "            d_loss, g_loss = gan.train_both(x)   # gan에 전달\n",
    "      \n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "            \n",
    "        # 에포크가 매 10회가 진행될 때 마다 결과 이미지를 파일로 저장\n",
    "        if epoch % 10 == 0 or epoch  == epochs -1 :\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose = 0)\n",
    "            save_images(w, output_fold, epoch, index)\n",
    "            \n",
    "            d_loss_ll.append(d_loss_l)\n",
    "            g_loss_ll.append(g_loss_l)\n",
    "        \n",
    "    gan.generator.save_weights(output_fold + '/' +'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetext(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetext(output_fold + '/' + 'g_loss', g_loss_ll)    \n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
