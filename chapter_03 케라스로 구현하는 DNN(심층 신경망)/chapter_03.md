# Chapter 03 # 

**심층 신경망(Deep Neural Network)이란?**

- 은닉 계층을 많이 쌓아서 만든 인공지능 기술이다.

- 2장에서 다룬 ANN은 주로 은닉 계층 하나를 포함했다. 그러나 DNN은 수십에서 수백의 은닉 계층으로 구성되기도 한다.

- 그덕분에 더 우수한 성능을 낼 수 있으며 적용 분야도 훨씬 다양하다.

## 3.1 DNN 원리 ## 

### 3.1.1 DNN 개념과 구조 ###

DNN은 은닉 계층이 여러 개인 신경망이다. 

구조는 다음 그림과 같다.

(사진)

2장의 ANN과 달리 제1 은닉 계층의 결과는 출력 계층이 아닌 제2 은닉 계층으로 들어간다.

또한 제2 은닉 계층의 결과도 이어지는 다음 은닉 계층으로 계속해서 들어갈 수 있다.

**이런 방식으로 다수의 은닉 계층을 활용하면 은닉 계층 하나를 사용할 때보다 입력 신호를 더 정교하게 처리할 수 있다.**

DNN은 이때문에 전체 노드 수가 늘어나 **과적합**이 될 수 있지만 최근 이를 효과적으로 해결하는 다양한 방식이 제시되었다.

DNN은 복잡도가 높은 비정형 빅데이터에 용이하지만 결국은 과적합을 얼마나 방지하느냐가 이를 제대로 활용하는 열쇠이다.

### 3.1.2 경사도 소실 문제와 ReLU 활성화 함수 ###

DNN을 포함한 인공신경망에 있어서 **경사도 소실 문제(vanishing gradient problem)** 에 대한 이해와 적절한 활성화 함수 선택은 최적화에 중요하다.

- 경사도 소실 문제

  - DNN은 여러 은닉 계층으로 구성되어 지능망의 최적화 과정에서 학습에 사용하는 활성화 함수에 따라 경사도 소실이 발생할 수 있다.

  - DNN은 여러 계층으로 구성되어 있고 각 계층 사이에 활성화 함수가 반복적으로 들어있어 오차역전파를 계산할 때 경사도 계산이 누적된다 => 이로인해 경사 하강법을 사용하는 오차역전파 알고리즘의 성능이 나빠질 수 있다.

  - 시그모이드 함수와 같이 입력을 특정 범위로 줄이는 활성화 함수들은 입력이 크면 경사도가 매우 작아져 경사도 소실을 유발할 가능성이 높다.

- ReLU 활성화 함수

  - DNN에서는 경사도 소실 문제를 극복하는 활성화 함수로 ReLU 등을 사용한다.

  - ReLU는 입력이 0보다 큰 구간에서는 직선 함수이기 때문에 값이 커져도 경사도를 구할 수 있다 => 따라서 **경사도 소실 문제에 덜 민감하다.**

### 3.1.3 DNN 구현 단계 ###

분류 DNN 구현을 다음과 같이 4단계로 구성해보았다.

이 장에서는 패키지 불러오기를 별도 단계로 수행하지 않고 각 단계에서 필요할 때 수행하여 단계별로 필요한 패키지를 각각 부르게 하여 어떤 단계에서 어느 패키지를 부르는지 확인하도록 한다.

< 구현 단계 >

1) 기본 파라미터 설정
2) 분류 DNN 모델 구현
3) 데이터 준비
4) DNN의 학습 및 성능 평가

## 3.2 필기체를 분류하는 DNN 구현 ##

이번에 사용할 데이터셋은 앞에서 사용한 0에서 9까지로 구분된 필기체 숫자들의 모음이다(5만 개의 학습데이터와 1만 개의 성능 평가 데이터로 구성).

위에서 말한 4단계로 은닉 계층이 늘어난 DNN을 구현한다.

### 3.2.1 기본 파라미터 설정 ###

1. DNN 구현에 필요한 파라미터 정의

       Nin = 784
       Nh_l = [100, 50]
       number_of_class = 10
       Nout = number_of_class

    - 입력 노드 수는 입력 이미지 크기에 해당하는 784개

    - 출력 노드 수는 분류할 클래스 수와 같은 10개

    - 은닉 계층은 두 개 이므로 각각에 대한 은닉 노드 수를 100과 50으로 지정

### 3.2.2 DNN 모델 구현 ###

2. DNN 모델 구현

   이번에는 객체지향 방식으로 DNN모델링 구현한다. 연쇄 방식으로 계층들을 기술할 것이므로 DNN 객체를 models.Sequential로부터 상속 받는다.

   모델링은 객체의 초기화 함수인 __init__()에서 구성한다.

        class DNN(models.Sequential):
          def __init__(self,Nin,Nh_l,Nout):
            super.__init__()

   - 연쇄 방식으로 구성할 것이므로 부모 클래스의 초기화 함수를 먼저 불러서 모델링이 시작됨을 알린다.

   - DNN의 은닉 계층과 출력 계층은 모두 케라스의 layers 서브패키지 아래에 Dense()개체로 구성한다.

         self.add(layers.Dense(Nh_l[0], activation = 'relu',input_shape = (Nin,), name = 'Hidden-1'))
         self.add(layers.Dropout(0.2))

   - 연쇄 방식으로 모델링을 기술하는 self.add()는 제1 은닉 계층부터 기술한다. 이름은 'Hidden-1'로 설정한다.

   - 입력 계층 정의는 첫 번째 은닉 계층의 정의와 함께 이루어진다. 첫 번째 은닉 계층 정의 시 input_shape을 적어줌으로 입력 계층이 Nin개의 노드로 구성된 벡터 방식임을 지정한다.

   - Dropout(p) : p라는 확률로 출력 노드의 신호를 보내다 말다 한다. p라는 확률로 앞 계층의 일부 노드들이 단절되기 때문에 훨씬 더 견고하게 신호에 적응한다.

         self.add(layers.Dense(Nh_l[1], activation = 'relu',input_shape = (Nin,), name = 'Hidden-2'))
         self.add(layers.Dropout(0.2))  
         self.add(layers.Dense(Nout,activation = 'softmax'))
    
   - 제2 은닉 계층을 제1 은닉 계층과 유사하게 구성한다(노드 수는 제1 은닉 계층의 노드 수와 다를 수 있다).

   - 이번에는 앞 계층의 노드 수를 적지 않았다. **제2 은닉 계층부터는 케라스가 자동으로 현재 계층의 입력 노드 수룰 앞에 나온 은닉 계층 출력 수로 설정해주기 때문이다.**

이제 모델 컴파일 할 단계이다.

    self.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    
   - 분류할 클래스 수가 2개 이상이므로 loss를 categorical_crossentropy로 설정하였고 최적화는 adam방식을 사용하였다.




