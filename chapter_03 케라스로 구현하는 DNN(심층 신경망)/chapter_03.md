# Chapter 03 # 

**심층 신경망(Deep Neural Network)이란?**

- 은닉 계층을 많이 쌓아서 만든 인공지능 기술이다.

- 2장에서 다룬 ANN은 주로 은닉 계층 하나를 포함했다. 그러나 DNN은 수십에서 수백의 은닉 계층으로 구성되기도 한다.

- 그덕분에 더 우수한 성능을 낼 수 있으며 적용 분야도 훨씬 다양하다.

## 3.1 DNN 원리 ## 

### 3.1.1 DNN 개념과 구조 ###

DNN은 은닉 계층이 여러 개인 신경망이다. 

구조는 다음 그림과 같다.

(사진)

2장의 ANN과 달리 제1 은닉 계층의 결과는 출력 계층이 아닌 제2 은닉 계층으로 들어간다.

또한 제2 은닉 계층의 결과도 이어지는 다음 은닉 계층으로 계속해서 들어갈 수 있다.

**이런 방식으로 다수의 은닉 계층을 활용하면 은닉 계층 하나를 사용할 때보다 입력 신호를 더 정교하게 처리할 수 있다.**

DNN은 이때문에 전체 노드 수가 늘어나 **과적합**이 될 수 있지만 최근 이를 효과적으로 해결하는 다양한 방식이 제시되었다.

DNN은 복잡도가 높은 비정형 빅데이터에 용이하지만 결국은 과적합을 얼마나 방지하느냐가 이를 제대로 활용하는 열쇠이다.

### 3.1.2 경사도 소실 문제와 ReLU 활성화 함수 ###

DNN을 포함한 인공신경망에 있어서 **경사도 소실 문제(vanishing gradient problem)** 에 대한 이해와 적절한 활성화 함수 선택은 최적화에 중요하다.

- 경사도 소실 문제

  - DNN은 여러 은닉 계층으로 구성되어 지능망의 최적화 과정에서 학습에 사용하는 활성화 함수에 따라 경사도 소실이 발생할 수 있다.

  - DNN은 여러 계층으로 구성되어 있고 각 계층 사이에 활성화 함수가 반복적으로 들어있어 오차역전파를 계산할 때 경사도 계산이 누적된다 => 이로인해 경사 하강법을 사용하는 오차역전파 알고리즘의 성능이 나빠질 수 있다.

  - 시그모이드 함수와 같이 입력을 특정 범위로 줄이는 활성화 함수들은 입력이 크면 경사도가 매우 작아져 경사도 소실을 유발할 가능성이 높다.

- ReLU 활성화 함수

  - DNN에서는 경사도 소실 문제를 극복하는 활성화 함수로 ReLU 등을 사용한다.

  - ReLU는 입력이 0보다 큰 구간에서는 직선 함수이기 때문에 값이 커져도 경사도를 구할 수 있다 => 따라서 **경사도 소실 문제에 덜 민감하다.**

### 3.1.3 DNN 구현 단계 ###

분류 DNN 구현을 다음과 같이 4단계로 구성해보았다.

이 장에서는 패키지 불러오기를 별도 단계로 수행하지 않고 각 단계에서 필요할 때 수행하여 단계별로 필요한 패키지를 각각 부르게 하여 어떤 단계에서 어느 패키지를 부르는지 확인하도록 한다.

< 구현 단계 >

1) 기본 파라미터 설정
2) 분류 DNN 모델 구현
3) 데이터 준비
4) DNN의 학습 및 성능 평가

## 3.2 필기체를 분류하는 DNN 구현 ##
















